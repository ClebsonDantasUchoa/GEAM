{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processamento de Linguagem Natural"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processamento inteligente de texto puro é algo muito difícil: muitas palavras são raras, palavras completamente distintas podem ter significados iguais enquanto a mesma palavra pode ter um significado completamente diferente dependendo do contexto.\n",
    "O objetivo do spaCy: pegar textos puros e retornar um objeto Doc, que contém diversas informações.\n",
    "\n",
    "Esse notebook foi inspirado nos códigos encontrados em: <http://leportella.com/pt-br/2017/11/30/brincando-de-nlp-com-spacy.html> e <https://www.analyticsvidhya.com/blog/2017/04/natural-language-processing-made-easy-using-spacy-%E2%80%8Bin-python/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instalação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ pip install spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ python -m spacy download en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ python -m spacy download pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usando um modelo pré-pronto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos usar direto o modelo de português que acabamos de baixar. Para isso, separamos o modelo e chamamos ele genericamente de nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir de agora podemos usar esse modelo para entender frases em português. Vamos usar o nlp para estudar uma frase simples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(u'Você encontrou o livro que eu te falei, Carla?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um Doc, objeto como aquele que acabamos de criar, é uma sequência de objetos do tipo Token e possui diversas informações sobre o texto que ele contém. Por dividir a frase em tokens, esse documento é uma estrutura iterável e portanto, deve ser acessada como tal. Já um Token é uma parte da estrutura e pode ser uma frase, palavra, uma pontuação, um espaço em branco, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Você', 'encontrou', 'o', 'livro', 'que', 'eu', 'te', 'falei,', 'Carla?']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Você, encontrou, o, livro, que, eu, te, falei, ,, Carla, ?]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[token for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Você',\n",
       " 'encontrou',\n",
       " 'o',\n",
       " 'livro',\n",
       " 'que',\n",
       " 'eu',\n",
       " 'te',\n",
       " 'falei',\n",
       " ',',\n",
       " 'Carla',\n",
       " '?']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[token.orth_ for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Você', 'encontrou', 'o', 'livro', 'que', 'eu', 'te', 'falei', 'Carla']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[token.orth_ for token in doc if not token.is_punct]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [token for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.29921815"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[0].similarity(tokens[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.067587666"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[0].similarity(tokens[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Você', 'PRON'),\n",
       " ('encontrou', 'VERB'),\n",
       " ('o', 'DET'),\n",
       " ('livro', 'NOUN'),\n",
       " ('que', 'PRON'),\n",
       " ('eu', 'PRON'),\n",
       " ('te', 'PRON'),\n",
       " ('falei', 'VERB'),\n",
       " (',', 'PUNCT'),\n",
       " ('Carla', 'PROPN'),\n",
       " ('?', 'PUNCT')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(token.orth_, token.pos_) for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['encontrar', 'falar']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[token.lemma_ for token in doc if token.pos_ == 'VERB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(u'encontrar encontrei')\n",
    "tokens = [token for token in doc]\n",
    "tokens[0].is_ancestor(tokens[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Machado de Assis, Brasil, Academia Brasileira de Letras)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(u'Machado de Assis um dos melhores escritores do Brasil, foi o primeiro presidente da Academia Brasileira de Letras')\n",
    "doc.ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Machado de Assis, 'PER'),\n",
       " (Brasil, 'LOC'),\n",
       " (Academia Brasileira de Letras, 'ORG')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(entity, entity.label_) for entity in doc.ents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Barack Obama, 'PERSON'), (American, 'NORP'), (\n",
       "       , 'PERSON'), (44th, 'ORDINAL'), (the United States,\n",
       "  'GPE'), (2009 to 2017, 'DATE'), (first, 'ORDINAL'), (\n",
       "       African American, 'NORP'), (\n",
       "       , 'PERSON'), (first, 'ORDINAL'), (United States, 'GPE')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_obama = \"\"\"Barack Obama is an American politician who served as\n",
    "     the 44th President of the United States from 2009 to 2017. He is the first\n",
    "     African American to have served as president,\n",
    "     as well as the first born outside the contiguous United States.\"\"\"\n",
    "nlp = spacy.load('en')\n",
    "nlp_obama = nlp(wiki_obama)\n",
    "[(i, i.label_) for i in nlp_obama.ents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizando spacy para um problema de classificação\n",
    "link para o dataset: <http://www.ppgia.pucpr.br/~paraiso/mineracaodeemocoes/formulario.php?download=Base_ENIAC.txt>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.base import TransformerMixin \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "import string\n",
    "punctuations = string.punctuation\n",
    "parser = spacy.load('pt')\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('portuguese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transformador customizado utilizando o spaCy \n",
    "class predictors(TransformerMixin):\n",
    "    def transform(self, X, **transform_params):\n",
    "        return [clean_text(text) for text in X]\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    def get_params(self, deep=True):\n",
    "        return {}\n",
    "\n",
    "# Função de utilidade básica para preprocessamento do texto\n",
    "def clean_text(text):     \n",
    "    return text.strip().lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cria tokenizador spacy que pega uma sentença e cria tokens\n",
    "#Este can also be replaced by word vectors \n",
    "def spacy_tokenizer(sentence):\n",
    "    tokens = parser(sentence)\n",
    "    tokens = [tok.lemma_.lower().strip() if tok.pos_ != \"PRON\" else tok.lower_ for tok in tokens]\n",
    "    tokens = [tok for tok in tokens if (tok not in stopwords and tok not in punctuations)]     \n",
    "    return tokens\n",
    "\n",
    "vectorizer = CountVectorizer(tokenizer = spacy_tokenizer, ngram_range=(1,1)) \n",
    "classifier = LinearSVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the  pipeline to clean, tokenize, vectorize, and classify \n",
    "pipe = Pipeline([(\"cleaner\", predictors()),\n",
    "                 ('vectorizer', vectorizer),\n",
    "                 ('classifier', classifier)])\n",
    "\n",
    "# Load sample data\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('base_eniac2.txt', names=['noticia', 'sentimento'],header=None, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>noticia</th>\n",
       "      <th>sentimento</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hospital de campanha começa a funcionar em Cax...</td>\n",
       "      <td>alegria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Estudo indica que chimpanzés são capazes de ap...</td>\n",
       "      <td>alegria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>STF nega ação que pedia a revogação de artigos...</td>\n",
       "      <td>alegria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Exército chinês faz demonstração de habilidade...</td>\n",
       "      <td>alegria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Zoológico alemão apresenta tigresas quadrigême...</td>\n",
       "      <td>alegria</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             noticia sentimento\n",
       "0  Hospital de campanha começa a funcionar em Cax...    alegria\n",
       "1  Estudo indica que chimpanzés são capazes de ap...    alegria\n",
       "2  STF nega ação que pedia a revogação de artigos...    alegria\n",
       "3  Exército chinês faz demonstração de habilidade...    alegria\n",
       "4  Zoológico alemão apresenta tigresas quadrigême...    alegria"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1750, 2)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentimento\n",
       "Surpresa      1\n",
       "alegria     250\n",
       "desgosto    250\n",
       "medo        250\n",
       "neutro      250\n",
       "raiva       250\n",
       "surpresa    249\n",
       "tristeza    250\n",
       "dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby('sentimento').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['sentimento'] = data.sentimento.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentimento\n",
       "alegria     250\n",
       "desgosto    250\n",
       "medo        250\n",
       "neutro      250\n",
       "raiva       250\n",
       "surpresa    250\n",
       "tristeza    250\n",
       "dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby('sentimento').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "noticias = data.values[:,0]\n",
    "labels = data.values[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = StratifiedKFold(n_splits=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6302857142857142"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(estimator=pipe,X=noticias, y=labels, cv=kfold.split(noticias,labels)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
