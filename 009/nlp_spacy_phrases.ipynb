{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLN Moderno em Python\n",
    "### _- Ou -_\n",
    "## O que podemos aprender sobre comida analizando 1 milhão de comentários do Yelp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Antes de começar...\n",
    "- Os exemplos desse notebook foram extraídos desse [notebook](http://nbviewer.jupyter.org/github/skipgram/modern-nlp-in-python/blob/master/executable/Modern_NLP_in_Python.ipynb#)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## O Dataset do Yelp\n",
    "[**The Yelp Dataset**](https://www.yelp.com/dataset_challenge/) é um conjunto de dados disponibilizado pelo serviço de comentários de estabelecimentos [Yelp](http://yelp.com) para fins de pesquisa academica e educacional.\n",
    "\n",
    "**Note:** Para executar esse notebook na sua máquina, você precisará baixar sua própria cópia do Yelp dataset. O dataset está pode ser baixado seguindo os passo abaixo:\n",
    "1. Vá a página do Yelp dataset [aqui](https://www.yelp.com/dataset_challenge/)\n",
    "1. Clique em \"Get the Data\"\n",
    "1. Leia e concorde com os termos de uso do Yelp.\n",
    "\n",
    "O atual conjunto de dados consiste em:\n",
    "- __552K__ usuários\n",
    "- __77K__ estabelecimentos\n",
    "- __2.2M__ comentários de usuários\n",
    "\n",
    "Quando queremos ver apenas os restaurantes, existem aproximadamente __55K__ restaurantes com aproximadamente __3.2M__ comentários de usuários escritas sobre eles.\n",
    "\n",
    "Os dados estão disponíveis em vários arquivos no formato _.json_. Usaremos os seguintes arquivos para a nossa demonstração:\n",
    "- __yelp\\_academic\\_dataset\\_business.json__ &mdash; _os registros de estabelecimentos individuais_\n",
    "- __yelp\\_academic\\_dataset\\_review.json__ &mdash; _os registros de comentários de usuarios escritos sobre os estabelecimentos_\n",
    "\n",
    "Os arquivos são arquivos de texto (UTF-8) com um _objeto json_ pot linha, cada um correspondendo para um registro de dado indivudual. Vamos ver uns exemplos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"business_id\": \"FYWN1wneV18bWNgQjJ2GNg\", \"name\": \"Dental by Design\", \"neighborhood\": \"\", \"address\": \"4855 E Warner Rd, Ste B9\", \"city\": \"Ahwatukee\", \"state\": \"AZ\", \"postal_code\": \"85044\", \"latitude\": 33.3306902, \"longitude\": -111.9785992, \"stars\": 4.0, \"review_count\": 22, \"is_open\": 1, \"attributes\": {\"AcceptsInsurance\": true, \"ByAppointmentOnly\": true, \"BusinessAcceptsCreditCards\": true}, \"categories\": [\"Dentists\", \"General Dentistry\", \"Health & Medical\", \"Oral Surgeons\", \"Cosmetic Dentists\", \"Orthodontists\"], \"hours\": {\"Friday\": \"7:30-17:00\", \"Tuesday\": \"7:30-17:00\", \"Thursday\": \"7:30-17:00\", \"Wednesday\": \"7:30-17:00\", \"Monday\": \"7:30-17:00\"}}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('dataset/business.json', encoding='utf_8') as f:\n",
    "    first_business_record = f.readline() \n",
    "\n",
    "print (first_business_record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os registros de estabelecimentos consistem de pares de _key: value_  contendo informações sobre o estabelecimento. Alguns atributos que estaremos interessados nessa demonstração incluem:\n",
    "- __business\\_id__ &mdash; _identificador único de um estabelecimento_\n",
    "- __categories__ &mdash; _Um array que contém as categorias que o estabelecimento se encaixa_\n",
    "\n",
    "O atributo de _categorias_ É de interesse especial. Esta demonstração foca em restaurantes, que são indicados pela presença da tag  _Restaurant_ no _array_ de categorias _categories_ . Além disso, o _array_ _categories_ pode contar mais informações detalhadas sobre restaurantes, como por exemplo o tipo de comida eles servem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os registros de comentários estão ordenados de maneira similar &mdash; pares de _key: value_ contendo informação sobre quem fez o comentário, comentário em si, e qual estabelecimento aquele comentário se refere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"review_id\":\"v0i_UHJMo_hPBq9bxWvW4w\",\"user_id\":\"bv2nCi5Qv5vroFiqKGopiw\",\"business_id\":\"0W4lkclzZThpx3V65bVgig\",\"stars\":5,\"date\":\"2016-05-28\",\"text\":\"Love the staff, love the meat, love the place. Prepare for a long line around lunch or dinner hours. \\n\\nThey ask you how you want you meat, lean or something maybe, I can't remember. Just say you don't want it too fatty. \\n\\nGet a half sour pickle and a hot pepper. Hand cut french fries too.\",\"useful\":0,\"funny\":0,\"cool\":0}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('dataset/review.json', encoding='utf_8') as f:\n",
    "    first_review_record = f.readline()\n",
    "    \n",
    "print (first_review_record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alguns atributos que podemos notar no registro de comentário são:\n",
    "- __business\\_id__ &mdash; _identificador único de um estabelecimento_\n",
    "- __text__ &mdash; _o texto de linguagem natural o usuário escreveu_\n",
    "\n",
    "O atributo _text_ será nosso foco!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_json_ é um formato bastante útil para troca de dados, porém não é muito utilizado para trabalhos de modelagem. Vamos aplicar um pré processamento para transoformar os dados em um formato mais usável. O próximo bloco de código fará:\n",
    "1. Ler cada registro de estabelecimento e converter para um `dict` do Python\n",
    "2. Descartar os estabelecimentos que não são restaurantes.\n",
    "3. Criar um `frozenset` dos business IDs para restaurantes, que será usado no próximo passo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54,618 restaurantes no dataset.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "restaurant_ids = set()\n",
    "\n",
    "# open the businesses file\n",
    "with open('dataset/business.json', encoding='utf_8') as f:\n",
    "    \n",
    "    # iterate through each line (json record) in the file\n",
    "    for business_json in f:\n",
    "        \n",
    "        # convert the json record to a Python dict\n",
    "        business = json.loads(business_json)\n",
    "        \n",
    "        # if this business is not a restaurant, skip to the next one\n",
    "        if u'Restaurants' not in business[u'categories']:\n",
    "            continue\n",
    "            \n",
    "        # add the restaurant business id to our restaurant_ids set\n",
    "        restaurant_ids.add(business[u'business_id'])\n",
    "\n",
    "# turn restaurant_ids into a frozenset, as we don't need to change it anymore\n",
    "restaurant_ids = frozenset(restaurant_ids)\n",
    "\n",
    "# print the number of unique restaurant ids in the dataset\n",
    "print ('{:,}'.format(len(restaurant_ids)), u'restaurantes no dataset.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No próximo, iremos criar um arquivo que contém apenas os comentários sobre restaurantes, com um comentário por linha no arquivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text from 3,223,214 restaurant reviews in the txt file.\n",
      "CPU times: user 6.38 s, sys: 2.18 s, total: 8.56 s\n",
      "Wall time: 9.76 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# this is a bit time consuming - make the if statement True\n",
    "# if you want to execute data prep yourself.\n",
    "if 0 == 1:\n",
    "    \n",
    "    review_count = 0\n",
    "\n",
    "    # create & open a new file in write mode\n",
    "    with open('review.txt', 'w', encoding='utf_8') as review_txt_file:\n",
    "\n",
    "        # open the existing review json file\n",
    "        with open('dataset/review.json', encoding='utf_8') as review_json_file:\n",
    "\n",
    "            # loop through all reviews in the existing file and convert to dict\n",
    "            for review_json in review_json_file:\n",
    "                review = json.loads(review_json)\n",
    "\n",
    "                # if this review is not sobre a restaurant, skip to the next one\n",
    "                if review[u'business_id'] not in restaurant_ids:\n",
    "                    continue\n",
    "\n",
    "                # write the restaurant review as a line in the new file\n",
    "                # escape newline characters in the original review text\n",
    "                review_txt_file.write(review[u'text'].replace('\\n', '\\\\n') + '\\n')\n",
    "                review_count += 1\n",
    "\n",
    "    print (u'''Text from {:,} restaurant reviews\n",
    "              escritas to the new txt file.'''.format(review_count))\n",
    "    \n",
    "else:\n",
    "    \n",
    "    with open('review.txt', encoding='utf_8') as review_txt_file:\n",
    "        for review_count, line in enumerate(review_txt_file):\n",
    "            pass\n",
    "        \n",
    "    print ('Text from {:,} restaurant reviews in the txt file.'.format(review_count + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spaCy &mdash; Industrial-Strength NLP in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![spaCy](https://s3.amazonaws.com/skipgram-images/spaCy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**spaCy**](https://spacy.io) É uma biblioteca para processamento de linguagem natural com Python. O objetivo do spaCy's é pegar avanços recentes em processamento de linguagem natual a partir de pesquisas e colocá-las em prática.\n",
    "\n",
    "spaCy suporta muitas tarefas associadas com a construção de pré-processamento em linguagem natural:\n",
    "- Tokenization\n",
    "- Text normalization, como converter para minusculo, radicalização\n",
    "- marcação Part-of-speech\n",
    "- Análise de dependência sintática\n",
    "- Detecção de limite de sentença\n",
    "- Reconhecimento e anotação de entidade nomeada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import itertools as it\n",
    "\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos pegar uma amostra de comentário para brincar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Love coming here. Yes the place always needs the floor swept but when you give out  peanuts in the shell how won't it always be a bit dirty. \n",
      "\n",
      "The food speaks for itself, so good. Burgers are made to order and the meat is put on the grill when you order your sandwich. Getting the small burger just means 1 patty, the regular is a 2 patty burger which is twice the deliciousness. \n",
      "\n",
      "Getting the Cajun fries adds a bit of spice to them and whatever size you order they always throw more fries (a lot more fries) into the bag.\n",
      "\n",
      "Had their chocolate almond croissant and it was amazing! So light and buttery and oh my how chocolaty.\n",
      "\n",
      "If you're looking for a light breakfast then head out here. Perfect spot for a coffee/latté before heading out to the old port\n",
      "\n",
      "Who would have guess that you would be able to get fairly decent Vietnamese restaurant in East York? \n",
      "\n",
      "Not quite the same as Chinatown in terms of pricing (slightly higher) but definitely one of the better Vietnamese restaurants outside of the neighbourhood. When I don't have time to go to Chinatown, this is the next best thing as it is down the street from me.\n",
      "\n",
      "So far the only items I have tried are the phos (beef, chicken & vegetarian) - and they have not disappointed me! Especially the chicken pho.\n",
      "\n",
      "Next time I go back, I'm going to try the banh cuon (steamed rice noodle) and the vermicelli!\n",
      "\n",
      "Not bad!! Love that there is a gluten-free, vegan version of the cheese curds and gravy!!\n",
      "\n",
      "Haven't done the poutine taste test yet with smoke's but Im excited to see which is better. However poutini's might win as they are vegan and gluten-free\n",
      "\n",
      "This is currently my parents new favourite restaurant. \n",
      "\n",
      "We come here in the morning for dim sum. They are not the cart pushing type of dim sum, it is order off of the sheet. Dim sum is not bad and not expensive either.\n",
      "\n",
      "We also frequent the dinner scene. Their set dinner menu is not bad. We typically order a 6 dish menu and it's big enough to feed a family of 9 with leftovers. \n",
      "\n",
      "Overall, food is pretty tasty!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('review.txt', encoding='utf_8') as f:\n",
    "    sample_review = '\\n'.join(list(it.islice(f, 4, 9)))\n",
    "    sample_review = sample_review.replace('\\\\n', '\\n')\n",
    "        \n",
    "print (sample_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos mandar esses comentários para o spacy..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 146 ms, sys: 41.2 ms, total: 187 ms\n",
      "Wall time: 154 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "parsed_review = nlp(sample_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Love coming here. Yes the place always needs the floor swept but when you give out  peanuts in the shell how won't it always be a bit dirty. \n",
      "\n",
      "The food speaks for itself, so good. Burgers are made to order and the meat is put on the grill when you order your sandwich. Getting the small burger just means 1 patty, the regular is a 2 patty burger which is twice the deliciousness. \n",
      "\n",
      "Getting the Cajun fries adds a bit of spice to them and whatever size you order they always throw more fries (a lot more fries) into the bag.\n",
      "\n",
      "Had their chocolate almond croissant and it was amazing! So light and buttery and oh my how chocolaty.\n",
      "\n",
      "If you're looking for a light breakfast then head out here. Perfect spot for a coffee/latté before heading out to the old port\n",
      "\n",
      "Who would have guess that you would be able to get fairly decent Vietnamese restaurant in East York? \n",
      "\n",
      "Not quite the same as Chinatown in terms of pricing (slightly higher) but definitely one of the better Vietnamese restaurants outside of the neighbourhood. When I don't have time to go to Chinatown, this is the next best thing as it is down the street from me.\n",
      "\n",
      "So far the only items I have tried are the phos (beef, chicken & vegetarian) - and they have not disappointed me! Especially the chicken pho.\n",
      "\n",
      "Next time I go back, I'm going to try the banh cuon (steamed rice noodle) and the vermicelli!\n",
      "\n",
      "Not bad!! Love that there is a gluten-free, vegan version of the cheese curds and gravy!!\n",
      "\n",
      "Haven't done the poutine taste test yet with smoke's but Im excited to see which is better. However poutini's might win as they are vegan and gluten-free\n",
      "\n",
      "This is currently my parents new favourite restaurant. \n",
      "\n",
      "We come here in the morning for dim sum. They are not the cart pushing type of dim sum, it is order off of the sheet. Dim sum is not bad and not expensive either.\n",
      "\n",
      "We also frequent the dinner scene. Their set dinner menu is not bad. We typically order a 6 dish menu and it's big enough to feed a family of 9 with leftovers. \n",
      "\n",
      "Overall, food is pretty tasty!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (parsed_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parece a mesma coisa! O que aconteceu por baixo dos panos?\n",
    "\n",
    "Sobre detecção de sentença e segmentação?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1:\n",
      "Love coming here.\n",
      "\n",
      "Sentence 2:\n",
      "Yes the place always needs the floor swept but when you give out  peanuts in the shell how won't it always be a bit dirty. \n",
      "\n",
      "\n",
      "\n",
      "Sentence 3:\n",
      "The food speaks for itself, so good.\n",
      "\n",
      "Sentence 4:\n",
      "Burgers are made to order and the meat is put on the grill when you order your sandwich.\n",
      "\n",
      "Sentence 5:\n",
      "Getting the small burger just means 1 patty, the regular is a 2 patty burger which is twice the deliciousness. \n",
      "\n",
      "\n",
      "\n",
      "Sentence 6:\n",
      "Getting the Cajun fries adds a bit of spice to them and whatever size you order they always throw more fries (a lot more fries) into the bag.\n",
      "\n",
      "\n",
      "\n",
      "Sentence 7:\n",
      "Had their chocolate almond croissant and it was amazing!\n",
      "\n",
      "Sentence 8:\n",
      "So light and buttery\n",
      "\n",
      "Sentence 9:\n",
      "and oh my how chocolaty.\n",
      "\n",
      "\n",
      "\n",
      "Sentence 10:\n",
      "If you're looking for a light breakfast then head out here.\n",
      "\n",
      "Sentence 11:\n",
      "Perfect spot for a coffee/latté before heading out to the old port\n",
      "\n",
      "Who would have guess that you would be able to get fairly decent Vietnamese restaurant in East York? \n",
      "\n",
      "\n",
      "\n",
      "Sentence 12:\n",
      "Not quite the same as Chinatown in terms of pricing (slightly higher) but definitely one of the better Vietnamese restaurants outside of the neighbourhood.\n",
      "\n",
      "Sentence 13:\n",
      "When I don't have time to go to Chinatown, this is the next best thing as it is down the street from me.\n",
      "\n",
      "\n",
      "\n",
      "Sentence 14:\n",
      "So far the only items I have tried are the phos (beef, chicken & vegetarian) - and they have not disappointed me!\n",
      "\n",
      "Sentence 15:\n",
      "Especially the chicken pho.\n",
      "\n",
      "\n",
      "\n",
      "Sentence 16:\n",
      "Next time I go back, I'm going to try the banh cuon (steamed rice noodle) and the vermicelli!\n",
      "\n",
      "\n",
      "\n",
      "Sentence 17:\n",
      "Not bad!!\n",
      "\n",
      "Sentence 18:\n",
      "Love that there is a gluten-free, vegan version of the cheese curds and gravy!!\n",
      "\n",
      "\n",
      "\n",
      "Sentence 19:\n",
      "Haven't done the poutine taste test yet with smoke's\n",
      "\n",
      "Sentence 20:\n",
      "but Im excited to see which is better.\n",
      "\n",
      "Sentence 21:\n",
      "However poutini's might win as they are vegan and gluten-free\n",
      "\n",
      "\n",
      "\n",
      "Sentence 22:\n",
      "This is currently my parents new favourite restaurant. \n",
      "\n",
      "\n",
      "\n",
      "Sentence 23:\n",
      "We come here in the morning for dim sum.\n",
      "\n",
      "Sentence 24:\n",
      "They are not the cart pushing type of dim sum, it is order off of the sheet.\n",
      "\n",
      "Sentence 25:\n",
      "Dim sum is not bad and not expensive either.\n",
      "\n",
      "\n",
      "\n",
      "Sentence 26:\n",
      "We also frequent the dinner scene.\n",
      "\n",
      "Sentence 27:\n",
      "Their set dinner menu is not bad.\n",
      "\n",
      "Sentence 28:\n",
      "We typically order a 6 dish menu and it's big enough to feed a family of 9 with leftovers. \n",
      "\n",
      "\n",
      "\n",
      "Sentence 29:\n",
      "Overall, food is pretty tasty!\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for num, sentence in enumerate(parsed_review.sents):\n",
    "    print ('Sentence {}:'.format(num + 1))\n",
    "    print (sentence)\n",
    "    print ('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E sobre reconhecimento de entidade nomeada?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity 1: 1 - CARDINAL\n",
      "\n",
      "Entity 2: 2 patty - QUANTITY\n",
      "\n",
      "Entity 3: Cajun - ORG\n",
      "\n",
      "Entity 4: Vietnamese - NORP\n",
      "\n",
      "Entity 5: East York - GPE\n",
      "\n",
      "Entity 6: one - CARDINAL\n",
      "\n",
      "Entity 7: Vietnamese - NORP\n",
      "\n",
      "Entity 8: the morning - TIME\n",
      "\n",
      "Entity 9: 6 - CARDINAL\n",
      "\n",
      "Entity 10: 9 - CARDINAL\n",
      "\n",
      "Entity 11: \n",
      " - GPE\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for num, entity in enumerate(parsed_review.ents):\n",
    "    print ('Entity {}:'.format(num + 1), entity, '-', entity.label_)\n",
    "    print ('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E sobre marcação part of the speach?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token_text</th>\n",
       "      <th>part_of_speech</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Love</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>coming</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>here</td>\n",
       "      <td>ADV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>.</td>\n",
       "      <td>PUNCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Yes</td>\n",
       "      <td>INTJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>the</td>\n",
       "      <td>DET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>place</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>always</td>\n",
       "      <td>ADV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>needs</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>the</td>\n",
       "      <td>DET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>floor</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>swept</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>but</td>\n",
       "      <td>CCONJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>when</td>\n",
       "      <td>ADV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>you</td>\n",
       "      <td>PRON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>give</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>out</td>\n",
       "      <td>PART</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td></td>\n",
       "      <td>SPACE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>peanuts</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>in</td>\n",
       "      <td>ADP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>the</td>\n",
       "      <td>DET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>shell</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>how</td>\n",
       "      <td>ADV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>wo</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>n't</td>\n",
       "      <td>ADV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>it</td>\n",
       "      <td>PRON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>always</td>\n",
       "      <td>ADV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>be</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>a</td>\n",
       "      <td>DET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>bit</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419</th>\n",
       "      <td>We</td>\n",
       "      <td>PRON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420</th>\n",
       "      <td>typically</td>\n",
       "      <td>ADV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421</th>\n",
       "      <td>order</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422</th>\n",
       "      <td>a</td>\n",
       "      <td>DET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423</th>\n",
       "      <td>6</td>\n",
       "      <td>NUM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424</th>\n",
       "      <td>dish</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425</th>\n",
       "      <td>menu</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426</th>\n",
       "      <td>and</td>\n",
       "      <td>CCONJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>427</th>\n",
       "      <td>it</td>\n",
       "      <td>PRON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428</th>\n",
       "      <td>'s</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>429</th>\n",
       "      <td>big</td>\n",
       "      <td>ADJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430</th>\n",
       "      <td>enough</td>\n",
       "      <td>ADV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431</th>\n",
       "      <td>to</td>\n",
       "      <td>PART</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432</th>\n",
       "      <td>feed</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>433</th>\n",
       "      <td>a</td>\n",
       "      <td>DET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>434</th>\n",
       "      <td>family</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>of</td>\n",
       "      <td>ADP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>436</th>\n",
       "      <td>9</td>\n",
       "      <td>NUM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>with</td>\n",
       "      <td>ADP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>leftovers</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>.</td>\n",
       "      <td>PUNCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>\\n\\n</td>\n",
       "      <td>SPACE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>Overall</td>\n",
       "      <td>ADV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442</th>\n",
       "      <td>,</td>\n",
       "      <td>PUNCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>443</th>\n",
       "      <td>food</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444</th>\n",
       "      <td>is</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445</th>\n",
       "      <td>pretty</td>\n",
       "      <td>ADV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446</th>\n",
       "      <td>tasty</td>\n",
       "      <td>ADJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447</th>\n",
       "      <td>!</td>\n",
       "      <td>PUNCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>448</th>\n",
       "      <td>\\n</td>\n",
       "      <td>SPACE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>449 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    token_text part_of_speech\n",
       "0         Love           NOUN\n",
       "1       coming           VERB\n",
       "2         here            ADV\n",
       "3            .          PUNCT\n",
       "4          Yes           INTJ\n",
       "5          the            DET\n",
       "6        place           NOUN\n",
       "7       always            ADV\n",
       "8        needs           VERB\n",
       "9          the            DET\n",
       "10       floor           NOUN\n",
       "11       swept           VERB\n",
       "12         but          CCONJ\n",
       "13        when            ADV\n",
       "14         you           PRON\n",
       "15        give           VERB\n",
       "16         out           PART\n",
       "17                      SPACE\n",
       "18     peanuts           NOUN\n",
       "19          in            ADP\n",
       "20         the            DET\n",
       "21       shell           NOUN\n",
       "22         how            ADV\n",
       "23          wo           VERB\n",
       "24         n't            ADV\n",
       "25          it           PRON\n",
       "26      always            ADV\n",
       "27          be           VERB\n",
       "28           a            DET\n",
       "29         bit           NOUN\n",
       "..         ...            ...\n",
       "419         We           PRON\n",
       "420  typically            ADV\n",
       "421      order           VERB\n",
       "422          a            DET\n",
       "423          6            NUM\n",
       "424       dish           NOUN\n",
       "425       menu           NOUN\n",
       "426        and          CCONJ\n",
       "427         it           PRON\n",
       "428         's           VERB\n",
       "429        big            ADJ\n",
       "430     enough            ADV\n",
       "431         to           PART\n",
       "432       feed           VERB\n",
       "433          a            DET\n",
       "434     family           NOUN\n",
       "435         of            ADP\n",
       "436          9            NUM\n",
       "437       with            ADP\n",
       "438  leftovers           NOUN\n",
       "439          .          PUNCT\n",
       "440       \\n\\n          SPACE\n",
       "441    Overall            ADV\n",
       "442          ,          PUNCT\n",
       "443       food           NOUN\n",
       "444         is           VERB\n",
       "445     pretty            ADV\n",
       "446      tasty            ADJ\n",
       "447          !          PUNCT\n",
       "448         \\n          SPACE\n",
       "\n",
       "[449 rows x 2 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_text = [token.orth_ for token in parsed_review]\n",
    "token_pos = [token.pos_ for token in parsed_review]\n",
    "\n",
    "pd.DataFrame(list(zip(token_text, token_pos)),\n",
    "             columns=['token_text', 'part_of_speech'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E sobre normalização do texto, como radicalização e análise de formato?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token_text</th>\n",
       "      <th>token_lemma</th>\n",
       "      <th>token_shape</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Love</td>\n",
       "      <td>love</td>\n",
       "      <td>Xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>coming</td>\n",
       "      <td>come</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>here</td>\n",
       "      <td>here</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>Xxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>xxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>place</td>\n",
       "      <td>place</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>always</td>\n",
       "      <td>always</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>needs</td>\n",
       "      <td>need</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>xxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>floor</td>\n",
       "      <td>floor</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>swept</td>\n",
       "      <td>sweep</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>but</td>\n",
       "      <td>but</td>\n",
       "      <td>xxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>when</td>\n",
       "      <td>when</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>you</td>\n",
       "      <td>-PRON-</td>\n",
       "      <td>xxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>give</td>\n",
       "      <td>give</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>out</td>\n",
       "      <td>out</td>\n",
       "      <td>xxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>peanuts</td>\n",
       "      <td>peanut</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>in</td>\n",
       "      <td>in</td>\n",
       "      <td>xx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>xxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>shell</td>\n",
       "      <td>shell</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>how</td>\n",
       "      <td>how</td>\n",
       "      <td>xxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>wo</td>\n",
       "      <td>will</td>\n",
       "      <td>xx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>n't</td>\n",
       "      <td>not</td>\n",
       "      <td>x'x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>it</td>\n",
       "      <td>-PRON-</td>\n",
       "      <td>xx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>always</td>\n",
       "      <td>always</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>be</td>\n",
       "      <td>be</td>\n",
       "      <td>xx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>bit</td>\n",
       "      <td>bit</td>\n",
       "      <td>xxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419</th>\n",
       "      <td>We</td>\n",
       "      <td>-PRON-</td>\n",
       "      <td>Xx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420</th>\n",
       "      <td>typically</td>\n",
       "      <td>typically</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421</th>\n",
       "      <td>order</td>\n",
       "      <td>order</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422</th>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424</th>\n",
       "      <td>dish</td>\n",
       "      <td>dish</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425</th>\n",
       "      <td>menu</td>\n",
       "      <td>menu</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426</th>\n",
       "      <td>and</td>\n",
       "      <td>and</td>\n",
       "      <td>xxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>427</th>\n",
       "      <td>it</td>\n",
       "      <td>-PRON-</td>\n",
       "      <td>xx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428</th>\n",
       "      <td>'s</td>\n",
       "      <td>be</td>\n",
       "      <td>'x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>429</th>\n",
       "      <td>big</td>\n",
       "      <td>big</td>\n",
       "      <td>xxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430</th>\n",
       "      <td>enough</td>\n",
       "      <td>enough</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431</th>\n",
       "      <td>to</td>\n",
       "      <td>to</td>\n",
       "      <td>xx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432</th>\n",
       "      <td>feed</td>\n",
       "      <td>feed</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>433</th>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>434</th>\n",
       "      <td>family</td>\n",
       "      <td>family</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>of</td>\n",
       "      <td>of</td>\n",
       "      <td>xx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>436</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>with</td>\n",
       "      <td>with</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>leftovers</td>\n",
       "      <td>leftover</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>\\n\\n</td>\n",
       "      <td>\\n\\n</td>\n",
       "      <td>\\n\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>Overall</td>\n",
       "      <td>overall</td>\n",
       "      <td>Xxxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442</th>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>443</th>\n",
       "      <td>food</td>\n",
       "      <td>food</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444</th>\n",
       "      <td>is</td>\n",
       "      <td>be</td>\n",
       "      <td>xx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445</th>\n",
       "      <td>pretty</td>\n",
       "      <td>pretty</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446</th>\n",
       "      <td>tasty</td>\n",
       "      <td>tasty</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447</th>\n",
       "      <td>!</td>\n",
       "      <td>!</td>\n",
       "      <td>!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>448</th>\n",
       "      <td>\\n</td>\n",
       "      <td>\\n</td>\n",
       "      <td>\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>449 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    token_text token_lemma token_shape\n",
       "0         Love        love        Xxxx\n",
       "1       coming        come        xxxx\n",
       "2         here        here        xxxx\n",
       "3            .           .           .\n",
       "4          Yes         yes         Xxx\n",
       "5          the         the         xxx\n",
       "6        place       place        xxxx\n",
       "7       always      always        xxxx\n",
       "8        needs        need        xxxx\n",
       "9          the         the         xxx\n",
       "10       floor       floor        xxxx\n",
       "11       swept       sweep        xxxx\n",
       "12         but         but         xxx\n",
       "13        when        when        xxxx\n",
       "14         you      -PRON-         xxx\n",
       "15        give        give        xxxx\n",
       "16         out         out         xxx\n",
       "17                                    \n",
       "18     peanuts      peanut        xxxx\n",
       "19          in          in          xx\n",
       "20         the         the         xxx\n",
       "21       shell       shell        xxxx\n",
       "22         how         how         xxx\n",
       "23          wo        will          xx\n",
       "24         n't         not         x'x\n",
       "25          it      -PRON-          xx\n",
       "26      always      always        xxxx\n",
       "27          be          be          xx\n",
       "28           a           a           x\n",
       "29         bit         bit         xxx\n",
       "..         ...         ...         ...\n",
       "419         We      -PRON-          Xx\n",
       "420  typically   typically        xxxx\n",
       "421      order       order        xxxx\n",
       "422          a           a           x\n",
       "423          6           6           d\n",
       "424       dish        dish        xxxx\n",
       "425       menu        menu        xxxx\n",
       "426        and         and         xxx\n",
       "427         it      -PRON-          xx\n",
       "428         's          be          'x\n",
       "429        big         big         xxx\n",
       "430     enough      enough        xxxx\n",
       "431         to          to          xx\n",
       "432       feed        feed        xxxx\n",
       "433          a           a           x\n",
       "434     family      family        xxxx\n",
       "435         of          of          xx\n",
       "436          9           9           d\n",
       "437       with        with        xxxx\n",
       "438  leftovers    leftover        xxxx\n",
       "439          .           .           .\n",
       "440       \\n\\n        \\n\\n        \\n\\n\n",
       "441    Overall     overall       Xxxxx\n",
       "442          ,           ,           ,\n",
       "443       food        food        xxxx\n",
       "444         is          be          xx\n",
       "445     pretty      pretty        xxxx\n",
       "446      tasty       tasty        xxxx\n",
       "447          !           !           !\n",
       "448         \\n          \\n          \\n\n",
       "\n",
       "[449 rows x 3 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_lemma = [token.lemma_ for token in parsed_review]\n",
    "token_shape = [token.shape_ for token in parsed_review]\n",
    "\n",
    "pd.DataFrame(list(zip(token_text, token_lemma, token_shape)),\n",
    "             columns=['token_text', 'token_lemma', 'token_shape'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E sobre analise de entidade a nível de token?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token_text</th>\n",
       "      <th>entity_type</th>\n",
       "      <th>inside_outside_begin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Love</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>coming</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>here</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>.</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>the</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>place</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>always</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>needs</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>the</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>floor</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>swept</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>but</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>when</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>you</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>give</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>out</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>peanuts</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>in</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>the</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>shell</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>how</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>wo</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>n't</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>it</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>always</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>be</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>a</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>bit</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419</th>\n",
       "      <td>We</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420</th>\n",
       "      <td>typically</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421</th>\n",
       "      <td>order</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422</th>\n",
       "      <td>a</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423</th>\n",
       "      <td>6</td>\n",
       "      <td>CARDINAL</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424</th>\n",
       "      <td>dish</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425</th>\n",
       "      <td>menu</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426</th>\n",
       "      <td>and</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>427</th>\n",
       "      <td>it</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428</th>\n",
       "      <td>'s</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>429</th>\n",
       "      <td>big</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430</th>\n",
       "      <td>enough</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431</th>\n",
       "      <td>to</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432</th>\n",
       "      <td>feed</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>433</th>\n",
       "      <td>a</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>434</th>\n",
       "      <td>family</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>of</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>436</th>\n",
       "      <td>9</td>\n",
       "      <td>CARDINAL</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>with</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>leftovers</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>.</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>\\n\\n</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>Overall</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442</th>\n",
       "      <td>,</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>443</th>\n",
       "      <td>food</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444</th>\n",
       "      <td>is</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445</th>\n",
       "      <td>pretty</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446</th>\n",
       "      <td>tasty</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447</th>\n",
       "      <td>!</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>448</th>\n",
       "      <td>\\n</td>\n",
       "      <td>GPE</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>449 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    token_text entity_type inside_outside_begin\n",
       "0         Love                                O\n",
       "1       coming                                O\n",
       "2         here                                O\n",
       "3            .                                O\n",
       "4          Yes                                O\n",
       "5          the                                O\n",
       "6        place                                O\n",
       "7       always                                O\n",
       "8        needs                                O\n",
       "9          the                                O\n",
       "10       floor                                O\n",
       "11       swept                                O\n",
       "12         but                                O\n",
       "13        when                                O\n",
       "14         you                                O\n",
       "15        give                                O\n",
       "16         out                                O\n",
       "17                                            O\n",
       "18     peanuts                                O\n",
       "19          in                                O\n",
       "20         the                                O\n",
       "21       shell                                O\n",
       "22         how                                O\n",
       "23          wo                                O\n",
       "24         n't                                O\n",
       "25          it                                O\n",
       "26      always                                O\n",
       "27          be                                O\n",
       "28           a                                O\n",
       "29         bit                                O\n",
       "..         ...         ...                  ...\n",
       "419         We                                O\n",
       "420  typically                                O\n",
       "421      order                                O\n",
       "422          a                                O\n",
       "423          6    CARDINAL                    B\n",
       "424       dish                                O\n",
       "425       menu                                O\n",
       "426        and                                O\n",
       "427         it                                O\n",
       "428         's                                O\n",
       "429        big                                O\n",
       "430     enough                                O\n",
       "431         to                                O\n",
       "432       feed                                O\n",
       "433          a                                O\n",
       "434     family                                O\n",
       "435         of                                O\n",
       "436          9    CARDINAL                    B\n",
       "437       with                                O\n",
       "438  leftovers                                O\n",
       "439          .                                O\n",
       "440       \\n\\n                                O\n",
       "441    Overall                                O\n",
       "442          ,                                O\n",
       "443       food                                O\n",
       "444         is                                O\n",
       "445     pretty                                O\n",
       "446      tasty                                O\n",
       "447          !                                O\n",
       "448         \\n         GPE                    B\n",
       "\n",
       "[449 rows x 3 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_entity_type = [token.ent_type_ for token in parsed_review]\n",
    "token_entity_iob = [token.ent_iob_ for token in parsed_review]\n",
    "\n",
    "pd.DataFrame(list(zip(token_text, token_entity_type, token_entity_iob)),\n",
    "             columns=['token_text', 'entity_type', 'inside_outside_begin'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sobre a variedade de outros atributos a nível de token:\n",
    "- stopword\n",
    "- pontuação\n",
    "- espaço\n",
    "- representa um número\n",
    "- faz parte do vocabulário nativo do spacy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>log_probability</th>\n",
       "      <th>stop?</th>\n",
       "      <th>punctuation?</th>\n",
       "      <th>whitespace?</th>\n",
       "      <th>number?</th>\n",
       "      <th>out of vocab.?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Love</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>coming</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>here</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>.</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Yes</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>the</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>place</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>always</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>needs</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>the</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>floor</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>swept</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>but</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>when</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>you</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>give</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>out</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td></td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>peanuts</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>in</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>the</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>shell</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>how</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>wo</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>n't</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>it</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>always</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>be</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>a</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>bit</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419</th>\n",
       "      <td>We</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420</th>\n",
       "      <td>typically</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421</th>\n",
       "      <td>order</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422</th>\n",
       "      <td>a</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423</th>\n",
       "      <td>6</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424</th>\n",
       "      <td>dish</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425</th>\n",
       "      <td>menu</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426</th>\n",
       "      <td>and</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>427</th>\n",
       "      <td>it</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428</th>\n",
       "      <td>'s</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>429</th>\n",
       "      <td>big</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430</th>\n",
       "      <td>enough</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431</th>\n",
       "      <td>to</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432</th>\n",
       "      <td>feed</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>433</th>\n",
       "      <td>a</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>434</th>\n",
       "      <td>family</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>of</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>436</th>\n",
       "      <td>9</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>with</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>leftovers</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>.</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>\\n\\n</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>Overall</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442</th>\n",
       "      <td>,</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>443</th>\n",
       "      <td>food</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444</th>\n",
       "      <td>is</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445</th>\n",
       "      <td>pretty</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446</th>\n",
       "      <td>tasty</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447</th>\n",
       "      <td>!</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>448</th>\n",
       "      <td>\\n</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>449 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          text  log_probability stop? punctuation? whitespace? number?  \\\n",
       "0         Love            -20.0                                          \n",
       "1       coming            -20.0                                          \n",
       "2         here            -20.0   Yes                                    \n",
       "3            .            -20.0                Yes                       \n",
       "4          Yes            -20.0                                          \n",
       "5          the            -20.0   Yes                                    \n",
       "6        place            -20.0                                          \n",
       "7       always            -20.0   Yes                                    \n",
       "8        needs            -20.0                                          \n",
       "9          the            -20.0   Yes                                    \n",
       "10       floor            -20.0                                          \n",
       "11       swept            -20.0                                          \n",
       "12         but            -20.0   Yes                                    \n",
       "13        when            -20.0   Yes                                    \n",
       "14         you            -20.0   Yes                                    \n",
       "15        give            -20.0   Yes                                    \n",
       "16         out            -20.0   Yes                                    \n",
       "17                        -20.0                            Yes           \n",
       "18     peanuts            -20.0                                          \n",
       "19          in            -20.0   Yes                                    \n",
       "20         the            -20.0   Yes                                    \n",
       "21       shell            -20.0                                          \n",
       "22         how            -20.0   Yes                                    \n",
       "23          wo            -20.0                                          \n",
       "24         n't            -20.0                                          \n",
       "25          it            -20.0   Yes                                    \n",
       "26      always            -20.0   Yes                                    \n",
       "27          be            -20.0   Yes                                    \n",
       "28           a            -20.0   Yes                                    \n",
       "29         bit            -20.0                                          \n",
       "..         ...              ...   ...          ...         ...     ...   \n",
       "419         We            -20.0                                          \n",
       "420  typically            -20.0                                          \n",
       "421      order            -20.0                                          \n",
       "422          a            -20.0   Yes                                    \n",
       "423          6            -20.0                                    Yes   \n",
       "424       dish            -20.0                                          \n",
       "425       menu            -20.0                                          \n",
       "426        and            -20.0   Yes                                    \n",
       "427         it            -20.0   Yes                                    \n",
       "428         's            -20.0                                          \n",
       "429        big            -20.0                                          \n",
       "430     enough            -20.0   Yes                                    \n",
       "431         to            -20.0   Yes                                    \n",
       "432       feed            -20.0                                          \n",
       "433          a            -20.0   Yes                                    \n",
       "434     family            -20.0                                          \n",
       "435         of            -20.0   Yes                                    \n",
       "436          9            -20.0                                    Yes   \n",
       "437       with            -20.0   Yes                                    \n",
       "438  leftovers            -20.0                                          \n",
       "439          .            -20.0                Yes                       \n",
       "440       \\n\\n            -20.0                            Yes           \n",
       "441    Overall            -20.0                                          \n",
       "442          ,            -20.0                Yes                       \n",
       "443       food            -20.0                                          \n",
       "444         is            -20.0   Yes                                    \n",
       "445     pretty            -20.0                                          \n",
       "446      tasty            -20.0                                          \n",
       "447          !            -20.0                Yes                       \n",
       "448         \\n            -20.0                            Yes           \n",
       "\n",
       "    out of vocab.?  \n",
       "0              Yes  \n",
       "1              Yes  \n",
       "2              Yes  \n",
       "3              Yes  \n",
       "4              Yes  \n",
       "5              Yes  \n",
       "6              Yes  \n",
       "7              Yes  \n",
       "8              Yes  \n",
       "9              Yes  \n",
       "10             Yes  \n",
       "11             Yes  \n",
       "12             Yes  \n",
       "13             Yes  \n",
       "14             Yes  \n",
       "15             Yes  \n",
       "16             Yes  \n",
       "17             Yes  \n",
       "18             Yes  \n",
       "19             Yes  \n",
       "20             Yes  \n",
       "21             Yes  \n",
       "22             Yes  \n",
       "23             Yes  \n",
       "24             Yes  \n",
       "25             Yes  \n",
       "26             Yes  \n",
       "27             Yes  \n",
       "28             Yes  \n",
       "29             Yes  \n",
       "..             ...  \n",
       "419            Yes  \n",
       "420            Yes  \n",
       "421            Yes  \n",
       "422            Yes  \n",
       "423            Yes  \n",
       "424            Yes  \n",
       "425            Yes  \n",
       "426            Yes  \n",
       "427            Yes  \n",
       "428            Yes  \n",
       "429            Yes  \n",
       "430            Yes  \n",
       "431            Yes  \n",
       "432            Yes  \n",
       "433            Yes  \n",
       "434            Yes  \n",
       "435            Yes  \n",
       "436            Yes  \n",
       "437            Yes  \n",
       "438            Yes  \n",
       "439            Yes  \n",
       "440            Yes  \n",
       "441            Yes  \n",
       "442            Yes  \n",
       "443            Yes  \n",
       "444            Yes  \n",
       "445            Yes  \n",
       "446            Yes  \n",
       "447            Yes  \n",
       "448            Yes  \n",
       "\n",
       "[449 rows x 7 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_attributes = [(token.orth_,\n",
    "                     token.prob,\n",
    "                     token.is_stop,\n",
    "                     token.is_punct,\n",
    "                     token.is_space,\n",
    "                     token.like_num,\n",
    "                     token.is_oov)\n",
    "                    for token in parsed_review]\n",
    "\n",
    "df = pd.DataFrame(token_attributes,\n",
    "                  columns=['text',\n",
    "                           'log_probability',\n",
    "                           'stop?',\n",
    "                           'punctuation?',\n",
    "                           'whitespace?',\n",
    "                           'number?',\n",
    "                           'out of vocab.?'])\n",
    "\n",
    "df.loc[:, 'stop?':'out of vocab.?'] = (df.loc[:, 'stop?':'out of vocab.?']\n",
    "                                       .applymap(lambda x: u'Yes' if x else u''))\n",
    "                                               \n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se o texto que você gostaria de processar é  um texto de objetivo geral (i.e., não faz parte de um domínio especifico, como literatura medicinal), spaCy está pronto para ser usado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelagem de frases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelagem de frases _Phrase modeling_ é uma abordagem para aprender combinações de palavras afim de juntalas em um único token. A formula utilizada para modelagem de frases é:\n",
    "\n",
    "$$\\frac{count(A\\ B) - count_{min}}{count(A) * count(B)} * N > threshold$$\n",
    "\n",
    "...where:\n",
    "* $count(A)$ número de vezes o token $A$ aparece no corpus\n",
    "* $count(B)$ número de vezes o token $B$ aparece no corpus\n",
    "* $count(A\\ B)$ número de vezes os tokens $A\\ B$ aparecem no corpus nessa ordem\n",
    "* $N$ tamanho do vocabulário\n",
    "* $count_{min}$ parêmetro definido pelo usuário para assegurar que frases ocorrem um número mínimo de vezes\n",
    "* $threshold$ parêmetro definido pelo usuário para controlar a força da relação entre dois tokens o modelo requer antes de aceitá-los como uma frase\n",
    "\n",
    "A biblioteca [**gensim**](https://radimrehurek.com/gensim/index.html) será usada para a modelagem de frases &mdash; a classe [**Phrases**](https://radimrehurek.com/gensim/models/phrases.html) em particular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Phrases\n",
    "from gensim.models.word2vec import LineSentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como estamos realizando a modelagem de frases, estaremos fazendo uma transformação de dados iterativa ao mesmo tempo. Nosso roteiro para preparação de dados inclui:\n",
    "\n",
    "1. Segmentar texto de comentários completos em frases e normalizar o texto\n",
    "1. Modelagem de frase de primeira ordem $\\rightarrow$ _aplicar modelo de frases de primeira ordem para transformar sentenças_\n",
    "1. Modelagem de frase de segunda ordem $\\rightarrow$ _aplicar modelo de frase de segunda ordem para transformar sentenças_\n",
    "1.Aplicar normalização de texto e modelo de frase de segunda ordem ao texto de revisões completas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primeiro, vamos definir algumas funções auxiliares que usaremos para normalização de texto. Em particular, a função geradora `lemmatized_sentence_corpus` usará spaCy para:\n",
    "- Iterar sobre as revisões de 3.2 milhões no `review.txt` nós criamos antes\n",
    "- Dividir os comentáios em frases individuais\n",
    "- Remover pontuação e espaços escessivos\n",
    "- Radicalizar o texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def punct_space(token):\n",
    "    \"\"\"\n",
    "    helper function to eliminate tokens\n",
    "    that are pure punctuation or whitespace\n",
    "    \"\"\"\n",
    "    \n",
    "    return token.is_punct or token.is_space\n",
    "\n",
    "def line_review(filename):\n",
    "    \"\"\"\n",
    "    generator function to read in reviews from the file\n",
    "    and un-escape the original line breaks in the text\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(filename, encoding='utf_8') as f:\n",
    "        for review in f:\n",
    "            yield review.replace('\\\\n', '\\n')\n",
    "            \n",
    "def lemmatized_sentence_corpus(filename):\n",
    "    \"\"\"\n",
    "    generator function to use spaCy to parse reviews,\n",
    "    lemmatize the text, and yield sentences\n",
    "    \"\"\"\n",
    "    \n",
    "    for parsed_review in nlp.pipe(line_review(filename),\n",
    "                                  batch_size=10000, n_threads=4):\n",
    "        \n",
    "        for sent in parsed_review.sents:\n",
    "            yield u' '.join([token.lemma_ for token in sent\n",
    "                             if not punct_space(token)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos usar o gerador `lemmatized_sentence_corpus` para iterar o texto de comentários original, segmentando as revisões em frases individuais e normalizando o texto. Vamos gravar esses dados de volta para um novo arquivo (`unigram_sentences_all`), com uma sentença normalizada por linha. Usaremos esses dados para aprender nossos modelos de frases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# this is a bit time consuming - make the if statement True\n",
    "# if you want to execute data prep yourself.\n",
    "if 1 == 1:\n",
    "\n",
    "    with open('unigram_sentences.txt', 'w', encoding='utf_8') as f:\n",
    "        for sentence in lemmatized_sentence_corpus('review.txt'):\n",
    "            f.write(sentence + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_sentences = LineSentence(unigram_sentences_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no it be not the best food in the world but the service greatly help the perception and it do not taste bad\n",
      "\n",
      "so back in the late 90 there use to be this super kick as cinnamon ice cream like an apple pie ice cream without the apple or the pie crust\n",
      "\n",
      "so delicious\n",
      "\n",
      "however now there be some shit tastic replacement that taste like vanilla ice cream with last year 's red hot in the middle totally gross\n",
      "\n",
      "fortunately our server be nice enough to warn me sobre the change and bring me a sample so i only have to suffer the death of a childhood memory rather than also have to pay for it\n",
      "\n",
      "the portion be big and fill just do not come for the ice cream\n",
      "\n",
      "i have pretty much be eat at various king pretty regularly since i be a child when my parent would take my sister and i into the fox chapel location often\n",
      "\n",
      "lately me and my girl have be visit the heidelburg location\n",
      "\n",
      "i love the food it really taste homemade much like something a grandmother would make complete with gob of butter and side dish\n",
      "\n",
      "price be low selection be great but do not expect fine dining by any mean\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for unigram_sentence in it.islice(unigram_sentences, 230, 240):\n",
    "    print (' '.join(unigram_sentence))\n",
    "    print ('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.91 s, sys: 3.14 s, total: 9.05 s\n",
      "Wall time: 11 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# this is a bit time consuming - make the if statement True\n",
    "# if you want to execute modeling yourself.\n",
    "if 0 == 1:\n",
    "\n",
    "    bigram_model = Phrases(unigram_sentences)\n",
    "\n",
    "    bigram_model.save('bigram_model.txt')\n",
    "    \n",
    "# load the finished model from disk\n",
    "bigram_model = Phrases.load(bigram_model_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 1e+03 ns, total: 5 µs\n",
      "Wall time: 8.11 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# this is a bit time consuming - make the if statement True\n",
    "# if you want to execute data prep yourself.\n",
    "if 0 == 1:\n",
    "\n",
    "    with open('bigram_sentences.txt', 'w', encoding='utf_8') as f:\n",
    "        \n",
    "        for unigram_sentence in unigram_sentences:\n",
    "            \n",
    "            bigram_sentence = ' '.join(bigram_model[unigram_sentence])\n",
    "            \n",
    "            f.write(bigram_sentence + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigram_sentences = LineSentence(bigram_sentences_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no it be not the best food in the world but the service greatly help the perception and it do not taste bad\n",
      "\n",
      "so back in the late 90 there use to be this super kick as cinnamon ice_cream like an apple_pie ice_cream without the apple or the pie crust\n",
      "\n",
      "so delicious\n",
      "\n",
      "however now there be some shit tastic replacement that taste like vanilla_ice cream with last year 's red hot in the middle totally gross\n",
      "\n",
      "fortunately our server be nice enough to warn me sobre the change and bring me a sample so i only have to suffer the death of a childhood_memory rather_than also have to pay for it\n",
      "\n",
      "the portion be big and fill just do not come for the ice_cream\n",
      "\n",
      "i have pretty much be eat at various king pretty regularly since i be a child when my parent would take my sister and i into the fox_chapel location often\n",
      "\n",
      "lately me and my girl have be visit the heidelburg location\n",
      "\n",
      "i love the food it really taste homemade much like something a grandmother would make complete with gob of butter and side dish\n",
      "\n",
      "price be low selection be great but do not expect fine_dining by any mean\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for bigram_sentence in it.islice(bigram_sentences, 230, 240):\n",
    "    print (' '.join(bigram_sentence))\n",
    "    print ('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.85 s, sys: 3.17 s, total: 8.02 s\n",
      "Wall time: 9.58 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# this is a bit time consuming - make the if statement True\n",
    "# if you want to execute modeling yourself.\n",
    "if 0 == 1:\n",
    "\n",
    "    trigram_model = Phrases(bigram_sentences)\n",
    "\n",
    "    trigram_model.save('trigram_model.txt')\n",
    "    \n",
    "# load the finished model from disk\n",
    "trigram_model = Phrases.load(trigram_model_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll apply our trained second-order phrase model to our first-order transformed sentences, write the results out to a new file, and explore a few of the second-order transformed sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8 µs, sys: 4 µs, total: 12 µs\n",
      "Wall time: 21.9 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# this is a bit time consuming - make the if statement True\n",
    "# if you want to execute data prep yourself.\n",
    "if 0 == 1:\n",
    "\n",
    "    with open(\"trigram_sentences.txt\", 'w', encoding='utf_8') as f:\n",
    "        \n",
    "        for bigram_sentence in bigram_sentences:\n",
    "            \n",
    "            trigram_sentence = u' '.join(trigram_model[bigram_sentence])\n",
    "            \n",
    "            f.write(trigram_sentence + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trigram_sentences = LineSentence(trigram_sentences_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no it be not the best food in the world but the service greatly help the perception and it do not taste bad\n",
      "\n",
      "so back in the late 90 there use to be this super kick as cinnamon_ice_cream like an apple_pie ice_cream without the apple or the pie crust\n",
      "\n",
      "so delicious\n",
      "\n",
      "however now there be some shit tastic replacement that taste like vanilla_ice_cream with last year 's red hot in the middle totally gross\n",
      "\n",
      "fortunately our server be nice enough to warn me sobre the change and bring me a sample so i only have to suffer the death of a childhood_memory rather_than also have to pay for it\n",
      "\n",
      "the portion be big and fill just do not come for the ice_cream\n",
      "\n",
      "i have pretty much be eat at various king pretty regularly since i be a child when my parent would take my sister and i into the fox_chapel location often\n",
      "\n",
      "lately me and my girl have be visit the heidelburg location\n",
      "\n",
      "i love the food it really taste homemade much like something a grandmother would make complete with gob of butter and side dish\n",
      "\n",
      "price be low selection be great but do not expect fine_dining by any mean\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for trigram_sentence in it.islice(trigram_sentences, 230, 240):\n",
    "    print (' '.join(trigram_sentence))\n",
    "    print ('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5 µs, sys: 1e+03 ns, total: 6 µs\n",
      "Wall time: 11.9 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# this is a bit time consuming - make the if statement True\n",
    "# if you want to execute data prep yourself.\n",
    "if 1 == 1:\n",
    "\n",
    "    with open('trigram_reviews.txt', 'w', encoding='utf_8') as f:\n",
    "        \n",
    "        for parsed_review in nlp.pipe(line_review(review_txt_filepath),\n",
    "                                      batch_size=10000, n_threads=4):\n",
    "            \n",
    "            # lemmatize the text, removing punctuation and whitespace\n",
    "            unigram_review = [token.lemma_ for token in parsed_review\n",
    "                              if not punct_space(token)]\n",
    "            \n",
    "            # apply the first-order and second-order phrase models\n",
    "            bigram_review = bigram_model[unigram_review]\n",
    "            trigram_review = trigram_model[bigram_review]\n",
    "            \n",
    "            # remove any remaining stopwords\n",
    "            trigram_review = [term for term in trigram_review\n",
    "                              if term not in spacy.en.STOPWORDS]\n",
    "            \n",
    "            # write the transformed review as a line in the new file\n",
    "            trigram_review = u' '.join(trigram_review)\n",
    "            f.write(trigram_review + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's preview the results. We'll grab one review from the file with the original, untransformed text, grab the same review from the file with the normalized and transformed text, and compare the two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      "\n",
      "A great townie bar with tasty food and an interesting clientele. I went to check this place out on the way home from the airport one Friday night and it didn't disappoint. It is refreshing to walk into a townie bar and not feel like the music stops and everyone in the place is staring at you - I'm guessing the mixed crowd of older hockey fans, young men in collared shirts, and thirtysomethings have probably seen it all during their time at this place. \n",
      "\n",
      "The staff was top notch - the orders were somewhat overwhelming as they appeared short-staffed for the night, but my waitress tried to keep a positive attitude for my entire visit. The other waiter was wearing a hooded cardigan, and I wanted to steal it from him due to my difficulty in finding such a quality article of clothing.\n",
      "\n",
      "We ordered a white pizza - large in size, engulfed in cheese, full of garlic flavor, flavorful hot sausage. An overall delicious pizza, aside from 2 things: 1, way too much grease (I know this comes with the territory, but still, it is sometimes unbearable); 2, CANNED MUSHROOMS - the worst thing to come out of a can. Ever. I would rather eat canned Alpo than canned mushrooms. And if the mushrooms weren't canned, they were just the worst mushrooms I've ever consumed. The mushroom debacle is enough to lower the review by an entire star - disgusting!\n",
      "\n",
      "My advice for the place is keep everything awesome - random music from the jukebox, tasty food, great prices, good crowd and staff - and get some decent mushrooms; why they spoil an otherwise above average pie with such inferior crap, I'll never know.\n",
      "\n",
      "----\n",
      "\n",
      "Transformed:\n",
      "\n",
      "great townie bar tasty food interesting clientele check place way home airport friday_night disappoint refresh walk townie bar feel like music stop place star guess mixed crowd old hockey_fan young_man collared_shirt thirtysomethings probably time place staff top_notch order somewhat overwhelming appear short staff night waitress try positive_attitude entire visit waiter wear hooded cardigan want steal difficulty quality article clothing order white pizza large size engulf cheese garlic flavor flavorful hot sausage overall delicious pizza aside_from 2 thing 1 way grease know come territory unbearable 2 canned mushrooms bad thing come eat alpo canned_mushroom mushroom bad mushroom consume mushroom debacle lower review entire star disgusting advice place awesome random music jukebox tasty food great price good crowd staff decent mushroom spoil above_average pie inferior crap know\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print ('Original:' + '\\n')\n",
    "\n",
    "for review in it.islice(line_review(review_txt_filepath), 11, 12):\n",
    "    print (review)\n",
    "\n",
    "print ('----' + '\\n')\n",
    "print ('Transformed:' + '\\n')\n",
    "\n",
    "with codecs.open(trigram_reviews_filepath, encoding='utf_8') as f:\n",
    "    for review in it.islice(f, 11, 12):\n",
    "        print (review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that most of the grammatical structure has been scrubbed from the text &mdash; capitalization, articles/conjunctions, punctuation, spacing, etc. However, much of the general semantic *meaning* is still present. Also, multi-word concepts such as \"`friday_night`\" and \"`above_average`\" have been joined into single tokens, as expected. The review text is now ready for higher-level modeling. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
